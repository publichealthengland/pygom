{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameter Estimation: Optimization\n",
    "\n",
    "In the previous section, we've seen how to use Markov Chain Monte Carlo methods to estimate the posterior probability distribution of parameters.\n",
    "These methods have generally replaced older techniques which use algorithms to infer the most likely parameter values, with confidence intervals provided after making assumptions about the local likelihood landscape.\n",
    "PyGOM has the capability to estimate parameters in this way in addition to ABC.\n",
    "Although perhaps outdated, including these methods could still be practical if trying to reproduce work from older publications, for example.\n",
    "\n",
    "## Example: SEIR model (with I and R known)\n",
    "\n",
    "We now solve the same problem as in the previous section,this time via optimization of the cost fucntion.\n",
    "As a reminder: We have an SEIR model where time series of $I$ and $R$ are known and we wish to estimate $\\beta$, $\\alpha$ and $\\gamma$ with true values 0.35, 0.5 and 0.25 respectively.\n",
    "The total population, $N=10,000$ and the initial number infected, $i_0=5$, are both known.\n",
    "\n",
    "We start by loading the data again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efea520f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data\n",
    "import numpy as np\n",
    "\n",
    "out = np.loadtxt('seir_epi_data.txt')\n",
    "t=out[:,0]\n",
    "sol_i_r=out[:,1:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a5c30e7",
   "metadata": {},
   "source": [
    "The syntax for setting up the solver is similar to the approach via ABC.\n",
    "We first form our candidate SEIR model.\n",
    "Again, we must specify some values for $\\beta$, $\\alpha$ and $\\gamma$, even if we don't know them.\n",
    "Here we put zeros, but anything else would be acceptable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "857371e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pygom import common_models\n",
    "\n",
    "n_pop=1e4\n",
    "paramEval=[('beta', 0), ('alpha', 0), ('gamma', 0), ('N', n_pop)]\n",
    "ode_SEIR = common_models.SEIR_N_stochastic(param=paramEval)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80c83797",
   "metadata": {},
   "source": [
    "We provide an initial guess in `theta` and pass all relevant info, including initial conditions `x0`, to build an object of class `PoissonLoss`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ad9325",
   "metadata": {},
   "outputs": [],
   "source": [
    "theta = [0.4, 0.3, 0.3]  # initial guess\n",
    "\n",
    "i0=5\n",
    "x0=[n_pop-i0, 0, i0, 0]  # initial conditions\n",
    "\n",
    "from pygom import PoissonLoss\n",
    "\n",
    "objSEIR = PoissonLoss(theta, ode_SEIR,\n",
    "                      t0=t[0], x0=x0,\n",
    "                      t=t[1:], y=sol_i_r[1:,:],\n",
    "                      target_param=['beta', 'alpha', 'gamma'],\n",
    "                      state_name= ['I', 'R'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cabcb9ba",
   "metadata": {},
   "source": [
    "```{note}\n",
    "It is a good idea to check that value of $R_0$ corresponding to the initial guess is greater than 1.\n",
    "For instance, here $\\frac{\\beta}{\\gamma}=\\frac{0.4}{0.3}=1.33$.\n",
    "This is important because if $R_0<1$ then the resulting model output will be an exponential decay - essentially, a failed epidemic which will differ substantially from the data.\n",
    "If the optimization algorithm proceeds by attempting new sets of parameters in the neighbourhood of the initial ones, these are also likely to result in similarly bad model fits and the algorithm may not find a way out.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "140a5020",
   "metadata": {},
   "source": [
    "## Gradient calculations\n",
    "\n",
    "How the parameter optimizer will navigate parameter space will depend on the gradient of the cost function with respect to the parameters.\n",
    "PyGOM can calculate the gradient in two different ways, which are explained in more detail in {doc}`.gradient`\n",
    "First, let's compare the outputs of the two methods when evaluated at the initial condition, `theta`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de043dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(objSEIR.sensitivity())\n",
    "print(objSEIR.adjoint())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c47c6356",
   "metadata": {},
   "source": [
    "Also, let's compare the times each method takes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c74c5449",
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit objSEIR.sensitivity()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20856aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit objSEIR.adjoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a86770d",
   "metadata": {},
   "source": [
    "```{note}\n",
    "Note how we ran the two gradient functions once before timing it, that is because we only find the properties (Jacobian, gradient) of the ODEs during runtime.\n",
    "\n",
    "The amount of time taken for both method is dependent on the number of observations as well as the number of states.\n",
    "The effect on the adjoint method as the number of observations differs can be quite evident.\n",
    "This is because the adjoint method is under a discretization which loops in Python where as the forward sensitivity equations are solved via an integration.\n",
    "As the number of observation gets larger, the affect of the Python loop becomes more obvious.\n",
    "\n",
    "The difference in gradient is larger when there are less observations.\n",
    "This is because the adjoint method use interpolations on the output of the ode between each consecutive time points.\n",
    "Given solutions over the same length of time, fewer discretizations leads to a less accurate interpolation.\n",
    "Note that the interpolation is currently performed using a univariate spline, due to the limitation of Python packages.\n",
    "Ideally, one would prefer to use an (adaptive) Hermite or Chebyshev interpolation.\n",
    "\n",
    "#TODO refs\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "543ec840",
   "metadata": {},
   "source": [
    "## Optimized result\n",
    "\n",
    "We now employ optimization procedures which should progress from the initial guess (if the initial guess is sensible enough) to the parameter set which minimises our cost function.\n",
    "It is particularly important to set the boundaries for the parameters here, since we know that they are required to be positive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff093f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "boxBounds = [(0.0,2.0), (0.0,2.0), (0.0,2.0)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7590328f",
   "metadata": {},
   "source": [
    "Now we use the *SLSQP* optimization routine of `scipy.optimize` with gradient obtained by forward sensitivity.\n",
    "\n",
    "We could, of course, use other methods available in `scipy.optimize.minimize`, such as *L-BFGS-B* and *TNC*.\n",
    "We can also use methods that accepts the exact Hessian such as *trust-ncg* but that should not be necessary most of the time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83eca0ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize\n",
    "\n",
    "res = minimize(fun=objSEIR.cost,\n",
    "               jac=objSEIR.sensitivity,\n",
    "               x0=theta, \n",
    "               bounds=boxBounds,\n",
    "               method='SLSQP')\n",
    "\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "912700fc",
   "metadata": {},
   "source": [
    "We see that parameter values in agreement with the underlying model are recovered and there is visual agreement with data when plotted below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c826a9db",
   "metadata": {},
   "outputs": [],
   "source": [
    "pars=res.x\n",
    "\n",
    "ode_est = common_models.SEIR_N_stochastic([('beta', pars[0]), ('alpha', pars[1]), ('gamma', pars[2]), ('N', n_pop)])\n",
    "ode_est.initial_values = (x0, t[0])\n",
    "solution = ode_est.integrate(t[1::])\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "f, axarr = plt.subplots(1,2, layout='constrained', figsize=(10, 2.5))\n",
    "\n",
    "axarr[0].plot(t, sol_i_r[:,0], color='C0')\n",
    "axarr[0].plot(t, solution[:,2], color='C1')\n",
    "axarr[1].plot(t, sol_i_r[:,1], color='C0')\n",
    "axarr[1].plot(t, solution[:,3], color='C1');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f79d99fc",
   "metadata": {},
   "source": [
    "The problem is only partially solved, however, since we must now do more work to estimate CI's."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.15 ('sphinx-doc')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  },
  "vscode": {
   "interpreter": {
    "hash": "4dc1e323c80fe09539c74ad5c5a7c7d8d9ff99e04f7b3dbd3680daf878629d6e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
