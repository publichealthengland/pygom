{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameter Estimation: Example 1\n",
    "\n",
    "## Estimation under square loss\n",
    "\n",
    "To ease the estimation process when given data, a separate module\n",
    "{mod}`ode_loss` has been constructed for observations coming from a single\n",
    "state. We demonstrate how to do parametre fitting using two models; first, a the\n",
    "SIR model, followed by the Legrand SEIHFR model from {citets}`Legrand2007` [\\[Legrand2007\\]]() used \n",
    "for Ebola in the next page {doc}`.estimate2`.\n",
    "\n",
    "### SIR Model\n",
    "\n",
    "We set up an SIR model as seen previously in {doc}`.sir`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efea520f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pygom import SquareLoss, common_models\n",
    "\n",
    "import numpy\n",
    "\n",
    "import scipy.integrate\n",
    "\n",
    "import matplotlib.pyplot\n",
    "\n",
    "# define the parameters\n",
    "paramEval = [('beta',0.5), ('gamma',1.0/3.0)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbfac95a",
   "metadata": {},
   "source": [
    "Initialize the model using the preloaded common model {obj}`.SIR`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f09342a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ode = common_models.SIR(paramEval)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb03842",
   "metadata": {},
   "source": [
    "We assume that we have perfect information about the $R$\n",
    "compartment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb19871",
   "metadata": {},
   "outputs": [],
   "source": [
    "x0 = [1, 1.27e-6, 0]\n",
    "\n",
    "# time, including the initial time t0 at t=0\n",
    "\n",
    "t = numpy.linspace(0, 150, 1000)\n",
    "\n",
    "# determine the solution.\n",
    "\n",
    "solution = scipy.integrate.odeint(ode.ode, x0, t)\n",
    "\n",
    "#TODO why use scipy?\n",
    "\n",
    "y = solution[:,1:3].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a5c30e7",
   "metadata": {},
   "source": [
    "\n",
    "Initialize the class with some initial guess\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "857371e4",
   "metadata": {},
   "source": [
    "# our initial guess\n",
    "\n",
    "theta = [0.2, 0.2]\n",
    "\n",
    "objSIR = SquareLoss(theta, ode, x0, t[0], t[1::], y[1::,:], ['I','R'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cabcb9ba",
   "metadata": {},
   "source": [
    "\n",
    "Note that we need to provide the initial values, $x_{0}$ and $t_{0}$\n",
    "differently to the observations $y$ and the corresponding time $t$.\n",
    "Additionally, the state which the observation lines needs to be\n",
    "specified. Either a single state, or multiple states are allowed, as\n",
    "seen above.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "140a5020",
   "metadata": {},
   "source": [
    "\n",
    "### Difference in gradient\n",
    "\n",
    "We have provided two different ways of obtaining the gradient, these are\n",
    "explained in {doc}`.gradient` in a bit more detail. First, lets see how\n",
    "similar the outputs of the two methods are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de043dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "objSIR.sensitivity()\n",
    "\n",
    "objSIR.adjoint()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c47c6356",
   "metadata": {},
   "source": [
    "\n",
    "and the time required to obtain the gradient for the SIR model under\n",
    "$\\theta = (0.2,0.2)$, previously entered.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c74c5449",
   "metadata": {},
   "source": [
    "%timeit objSIR.sensitivity()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20856aa8",
   "metadata": {},
   "source": [
    "%timeit objSIR.adjoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a86770d",
   "metadata": {},
   "source": [
    "The amount of time taken for both method is dependent on the\n",
    "number of observations as well as the number of states. The effect on\n",
    "the adjoint method as the number of observations differs can be quite\n",
    "evident. This is because the adjoint method is under a discretization\n",
    "which loops in Python where as the forward sensitivity equations are\n",
    "solved via an integration. As the number of observation gets\n",
    "larger, the affect of the Python loop becomes more obvious.\n",
    "\n",
    "The difference in gradient is larger when there are less observations. This\n",
    "is because the adjoint method use interpolations on the output of the\n",
    "ode between each consecutive time points. Given solutions over the same\n",
    "length of time, fewer discretizations leads to a less accurate\n",
    "interpolation. Note that the interpolation is currently performed using a\n",
    "univariate spline, due to the limitation of Python packages. Ideally,\n",
    "one would prefer to use an (adaptive) Hermite or Chebyshev\n",
    "interpolation. \n",
    "\n",
    "#TODO add refs\n",
    "\n",
    "Note how we ran the two gradient functions once before\n",
    "timing it, that is because we only find the properties (Jacobian,\n",
    "gradient) of the ODEs during runtime.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "543ec840",
   "metadata": {},
   "source": [
    "\n",
    "### Optimized result\n",
    "\n",
    "Then standard optimization procedures with some suitable initial guess\n",
    "should yield the correct result. It is important to set the boundaries\n",
    "for compartmental models as we know that all the parameters are strictly\n",
    "positive. We put a less restrictive inequality here for demonstration\n",
    "purpose.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff093f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# what we think the bounds are\n",
    "\n",
    "boxBounds = [(0.0,2.0),(0.0,2.0)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7590328f",
   "metadata": {},
   "source": [
    "Then using the optimization routines in `scipy.optimize`, for example,\n",
    "the *SLSQP* method with the gradient obtained by forward sensitivity.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83eca0ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize\n",
    "\n",
    "res = minimize(fun=objSIR.cost, jac=objSIR.sensitivity, x0=theta, \n",
    "               bounds=boxBounds, method='SLSQP')\n",
    "\n",
    "print(res)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "404ddd1c",
   "metadata": {},
   "source": [
    "Other methods available in `scipy.optimize.minimize` can also be used,\n",
    "such as the *L-BFGS-B* and *TNC*. We can also use methods that accepts\n",
    "the exact Hessian such as *trust-ncg* but that should not be necessary\n",
    "most of the time.\n",
    "\n",
    "#TODO add doc refs for scipy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.15 ('sphinx-doc')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "4dc1e323c80fe09539c74ad5c5a7c7d8d9ff99e04f7b3dbd3680daf878629d6e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
